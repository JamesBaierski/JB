import os
from autogen import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager
from autogen.oai.client import OpenAI

# Base configuration for all agents
config_list = [{
    "model": "hermes-3-llama-3.2-3b",
    "base_url": 'http://127.0.0.1:1234',
    "api_key": "lm-studio"
}]

# Create specialized agents
Agent_1 = AssistantAgent(
    name="Agent_1",
    llm_config={
        "config_list": config_list,
        "temperature": 0.7,
    },
    system_message=
    '''
    

    '''
    
)

Agent_2 = AssistantAgent(
    name="Agent_2",
    llm_config={
        "config_list": config_list,
        "temperature": 0.7,
    },
    system_message=
    """
    

    """
)

Agent_3 = AssistantAgent(
    name="Agent_3",
    llm_config={
        "config_list": config_list,
        "temperature": 0.7,
    },
    system_message=
    
    """
    
    
    """
)

# Create user proxy with error handling
user_proxy = UserProxyAgent(
    name="UserProxy",
    human_input_mode="TERMINATE",
    max_consecutive_auto_reply= 1,
    code_execution_config={"use_docker": False}
)

# Set up the group chat with error handling
try:
    agents = [user_proxy, Agent_1 , Agent_2 , Agent_3]
    groupchat = GroupChat(agents=agents, messages=[], max_round = 4)
    manager = GroupChatManager(groupchat=groupchat, llm_config=config_list[0])

    # Start the conversation
    user_proxy.initiate_chat(
        manager,
        message=
    """
    

    """
    )

except Exception as e:
    print(f"An error occurred during chat initialization: {str(e)}")
