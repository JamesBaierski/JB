import autogen
import os
from typing import List, Tuple
from langchain.text_splitter import RecursiveCharacterTextSplitter
import logging

# Set up logging
logging.basicConfig(level=logging.INFO,
                   format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# Configuration for LM Studio local endpoint
config_list = [
    {
        'model': 'auto-rag-llama-3-8b-instruct-i1',
        'api_key': 'not-needed',
        'base_url': 'http://localhost:1234/v1',
        'api_type': 'open_ai'
    }
]

llm_config = {
    "config_list": config_list,
    "temperature": 0.7,
    "max_tokens": 1024  # Increased from 512
}

class DocumentStore:
    def __init__(self, chunk_size: int = 500, chunk_overlap: int = 50):
        self.documents = []
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            length_function=len,
            is_separator_regex=False
        )
        logger.info(f"Initialized DocumentStore with chunk_size={chunk_size}, overlap={chunk_overlap}")
    
    def add_documents(self, documents: List[str]) -> None:
        """Add documents to the store with detailed logging."""
        original_count = len(self.documents)
        for idx, doc in enumerate(documents):
            if not doc.strip():
                logger.warning(f"Document {idx} is empty or whitespace only")
                continue
                
            try:
                chunks = self.text_splitter.split_text(doc)
                logger.info(f"Document {idx}: Split into {len(chunks)} chunks")
                logger.debug(f"First chunk sample: {chunks[0][:100]}...")
                self.documents.extend(chunks)
            except Exception as e:
                logger.error(f"Error processing document {idx}: {str(e)}")
        
        new_count = len(self.documents) - original_count
        logger.info(f"Added {new_count} new chunks to document store")
    
    def search(self, query: str, k: int = 3) -> List[Tuple[float, str]]:
        """Enhanced search with scoring and logging."""
        if not self.documents:
            logger.warning("Document store is empty")
            return []
            
        query_terms = query.lower().split()
        relevant_chunks = []
        
        logger.info(f"Searching for query: '{query}' among {len(self.documents)} chunks")
        
        for idx, chunk in enumerate(self.documents):
            # Enhanced scoring: consider term frequency and chunk length
            chunk_lower = chunk.lower()
            score = sum(chunk_lower.count(term) for term in query_terms)
            if score > 0:
                # Normalize score by chunk length to avoid bias towards longer chunks
                normalized_score = score / (len(chunk_lower.split()) + 1)
                relevant_chunks.append((normalized_score, chunk))
                
        relevant_chunks.sort(reverse=True)
        selected_chunks = relevant_chunks[:k]
        
        logger.info(f"Found {len(selected_chunks)} relevant chunks")
        for idx, (score, chunk) in enumerate(selected_chunks):
            logger.debug(f"Chunk {idx} (score={score:.3f}): {chunk[:100]}...")
            
        return selected_chunks

def load_documents(file_paths: List[str]) -> List[str]:
    """Enhanced document loader with better error handling and PDF support."""
    try:
        import PyPDF2
    except ImportError:
        logger.error("PyPDF2 not installed. Installing required dependencies...")
        raise ImportError("Please install PyPDF2: pip install PyPDF2")
    
    documents = []
    
    for path in file_paths:
        if not os.path.exists(path):
            logger.error(f"File not found: {path}")
            continue
            
        logger.info(f"Processing file: {path}")
        
        if path.lower().endswith('.pdf'):
            try:
                with open(path, 'rb') as file:
                    pdf_reader = PyPDF2.PdfReader(file)
                    logger.info(f"PDF loaded: {len(pdf_reader.pages)} pages")
                    
                    text = []
                    for page_num, page in enumerate(pdf_reader.pages):
                        try:
                            page_text = page.extract_text()
                            if page_text.strip():
                                text.append(page_text)
                                logger.debug(f"Page {page_num}: Extracted {len(page_text)} characters")
                            else:
                                logger.warning(f"Page {page_num}: No text extracted")
                        except Exception as e:
                            logger.error(f"Error extracting text from page {page_num}: {str(e)}")
                    
                    if text:
                        combined_text = "\n".join(text)
                        documents.append(combined_text)
                        logger.info(f"Successfully extracted {len(combined_text)} characters from PDF")
                    else:
                        logger.warning(f"No text content extracted from PDF: {path}")
            except Exception as e:
                logger.error(f"Failed to process PDF {path}: {str(e)}")
        else:
            # Handle text files with multiple encodings
            encodings = ['utf-8', 'latin-1', 'cp1252', 'iso-8859-1']
            success = False
            
            for encoding in encodings:
                try:
                    with open(path, 'r', encoding=encoding) as file:
                        content = file.read()
                        if content.strip():
                            documents.append(content)
                            logger.info(f"Successfully read text file with {encoding}: {path}")
                            success = True
                            break
                        else:
                            logger.warning(f"File is empty: {path}")
                except UnicodeDecodeError:
                    logger.debug(f"Failed to decode with {encoding}, trying next encoding")
                    continue
                except Exception as e:
                    logger.error(f"Error reading file {path}: {str(e)}")
                    break
            
            if not success:
                logger.error(f"Failed to read {path} with all attempted encodings")
    
    if not documents:
        raise ValueError("No documents were successfully loaded")
    
    logger.info(f"Successfully loaded {len(documents)} documents")
    return documents

def create_assistant(doc_store: DocumentStore):
    """Create an assistant with enhanced context retrieval."""
    system_message = """You are a helpful assistant that answers questions based on the provided context. 
    Analyze the retrieved information carefully and provide accurate, detailed answers. 
    If the context is insufficient or unclear, explain what additional information might be needed."""
    
    async def retrieve_context(message: str) -> str:
        relevant_chunks = doc_store.search(message)
        if relevant_chunks:
            context_text = "\n\nContext:\n" + "\n---\n".join(chunk for _, chunk in relevant_chunks)
            logger.info(f"Retrieved {len(relevant_chunks)} relevant chunks for query")
            return context_text
        logger.warning("No relevant context found for query")
        return "\n\nNo relevant context found for the given query."
    
    return autogen.AssistantAgent(
        name="rag_assistant",
        system_message=system_message,
        llm_config=llm_config,
        function_map={"retrieve_context": retrieve_context}
    )

def create_user_proxy():
    """Create a user proxy with enhanced configuration."""
    return autogen.UserProxyAgent(
        name="user_proxy",
        human_input_mode="TERMINATE",
        max_consecutive_auto_reply=2,  # Increased from 1
        code_execution_config={
            "work_dir": "coding",
            "use_docker": False,
            "last_n_messages": 5,  # Increased context window
            "timeout": 180  # Increased timeout
        }
    )

def main():
    # Example usage with error handling
    try:
        document_paths = [r"C:\Users\J-Baierski\Desktop\LM Agent\data\combined_documentation.pdf"]
        
        logger.info("Starting document processing...")
        documents = load_documents(document_paths)
        
        logger.info("Initializing document store...")
        doc_store = DocumentStore(chunk_size=300, chunk_overlap=50)  # Adjusted chunk size
        doc_store.add_documents(documents)
        
        logger.info("Creating agents...")
        assistant = create_assistant(doc_store)
        user_proxy = create_user_proxy()
        
        logger.info("Starting chat interaction...")
        user_proxy.initiate_chat(
            assistant,
            message="Please analyze the help manual documentation and provide a detailed summary of the program."
        )
    except Exception as e:
        logger.error(f"Application error: {str(e)}", exc_info=True)
        raise

if __name__ == "__main__":
    main()
